# -*- coding: utf-8 -*-
"""「“tfMNISTNTNU_origin.ipynb」的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IiMv_BaSsqQAJd4V9wEbRmITTXj7XfuU
"""

import os
import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# This is the function for load the data
def filePathsGen(rootPath):
    paths=[]
    dirs=[]
    for dirPath,dirNames,fileNames in os.walk(rootPath):
        print(dirNames)
        for fileName in fileNames:
            fullPath=os.path.join(dirPath,fileName)
            #print(int(dirPath[len(rootPath)]))
            
            paths.append((int(dirPath[len(rootPath) ]),fullPath))
        dirs.append(dirNames)
    return dirs,paths

dirs,paths=filePathsGen('mnist/') #載入圖片路徑
print(paths)
print(dirs)

dfPath=pd.DataFrame(paths,columns=['class','path']) #圖片路徑存成Pandas資料表
dfPath.head(5) # 看資料表前5個row

"""###  利用Pandas, 可迅速了解每個資料夾裡面有幾張圖片"""

#依照class分群後，數各群的數量，並繪圖
dfCountPerClass=dfPath.groupby('class').size()
dfCountPerClass.rename({'path':'amount of figures'},inplace=True)
dfCountPerClass.plot(kind='bar',rot=0)

"""###  將圖片路徑資訊分成train($70\%$), val($10\%$), test($20\%$)"""

dfFiguresShuffled=dfPath.sample(frac=1) # randomized the path data

train=dfFiguresShuffled.sample(frac=0.8) # randomly pick 80% path data as training data
test=dfFiguresShuffled.drop(train.index) # 20% for test

trainVal=train.sample(frac=1/8) # 1/8 training data for validation
train=train.drop(trainVal.index)

#result
print('shape(df)=\t\t',dfPath.shape)
print('shape(train)=\t\t',train.shape)
print('shape(trainVal)=\t',trainVal.shape)
print('shape(test)=\t\t',test.shape)

# pick 10 figure
for j in range(10):
    img=plt.imread(train['path'].iloc[j])
    plt.imshow(img)
    plt.show()

"""###  load the figure as numpy array"""

def dataLoad(dfPath):
    x_paths=dfPath['path'].values
    y=dfPath['class'].values

    x=[]
    for x_path in x_paths:
        fig=plt.imread(x_path)
        x.append(fig)
    x=np.array(x)/255 # for normalize(scaling) the figure
    return x,y
#print(np.array([[1,2,3],[2,3,4],[3,4,5]])/5)

trainX,trainY=dataLoad(train)
trainValX,trainValY=dataLoad(trainVal)
testX,testY=dataLoad(test)

print('train:\t',trainX.shape,trainY.shape)
print('trainVal:',trainValX.shape,trainValY.shape)
print('test:\t',testX.shape,testY.shape) 
print(trainX,trainY)

"""[回索引](#Tensorflow手寫數字分類)

# <a id='2'> 2. model construction  </a>
"""

# parameter for model
batch_size = 128
learning_rate = 0.005
evaluation_size = 500
image_width = trainX[0].shape[0]
image_height = trainX[0].shape[1]
target_size = 10 # dimension of latent space
num_channels = 1 # greyscale = 1 channel
generation = 300
eval_every = 20
conv1_features = 25
conv2_features = 50
max_pool_size1 = 2 # NxN window for 1st max pool layer
max_pool_size2 = 2 # NxN window for 2nd max pool layer
fully_connected_size1 = 100

#parameter placeholder
x_input_shape = (batch_size, image_width, image_height, num_channels)
x_input = tf.placeholder(tf.float32, shape=x_input_shape) 
y_target = tf.placeholder(tf.int32, shape=(batch_size))

eval_input_shape = (evaluation_size, image_width, image_height, num_channels)
eval_input = tf.placeholder(tf.float32, shape=eval_input_shape)
eval_target = tf.placeholder(tf.int32, shape=(evaluation_size))

# 定義Convolutional layer variables
conv1_weight = tf.Variable(tf.truncated_normal([4, 4, num_channels, conv1_features],
                                               stddev=0.1, dtype=tf.float32))
conv1_bias = tf.Variable(tf.zeros([conv1_features], dtype=tf.float32))

conv2_weight = tf.Variable(tf.truncated_normal([4, 4, conv1_features, conv2_features],
                                               stddev=0.1, dtype=tf.float32))
conv2_bias = tf.Variable(tf.zeros([conv2_features], dtype=tf.float32))

# fully connected variables
resulting_width = image_width // (max_pool_size1 * max_pool_size2)
resulting_height = image_height // (max_pool_size1 * max_pool_size2)
full1_input_size = resulting_width * resulting_height * conv2_features
full1_weight = tf.Variable(tf.truncated_normal([full1_input_size, fully_connected_size1],
                          stddev=0.1, dtype=tf.float32))
full1_bias = tf.Variable(tf.truncated_normal([fully_connected_size1], stddev=0.1, dtype=tf.float32))
full2_weight = tf.Variable(tf.truncated_normal([fully_connected_size1, target_size],
                                               stddev=0.1, dtype=tf.float32))
full2_bias = tf.Variable(tf.truncated_normal([target_size], stddev=0.1, dtype=tf.float32))

"""### 模型建置"""

def my_conv_net(input_data):
    # First Conv-ReLU-MaxPool Layer
    conv1 = tf.nn.conv2d(input_data, conv1_weight, strides=[1, 1, 1, 1], padding='SAME')
    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))
    max_pool1 = tf.nn.max_pool(relu1, ksize=[1, max_pool_size1, max_pool_size1, 1],
                               strides=[1, max_pool_size1, max_pool_size1, 1], padding='SAME')

    # Second Conv-ReLU-MaxPool Layer
    conv2 = tf.nn.conv2d(max_pool1, conv2_weight, strides=[1, 1, 1, 1], padding='SAME')
    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_bias))
    max_pool2 = tf.nn.max_pool(relu2, ksize=[1, max_pool_size2, max_pool_size2, 1],
                               strides=[1, max_pool_size2, max_pool_size2, 1], padding='SAME')

    # Transform Output into a 1xN layer for next fully connected layer
    final_conv_shape = max_pool2.get_shape().as_list()
    final_shape = final_conv_shape[1] * final_conv_shape[2] * final_conv_shape[3]
    flat_output = tf.reshape(max_pool2, [final_conv_shape[0], final_shape])

    # First Fully Connected Layer
    fully_connected1 = tf.nn.relu(tf.add(tf.matmul(flat_output, full1_weight), full1_bias))

    # Second Fully Connected Layer
    final_model_output = tf.add(tf.matmul(fully_connected1, full2_weight), full2_bias)
    
    return(final_model_output)


model_output = my_conv_net(x_input)
val_model_output = my_conv_net(eval_input)

# Loss function, here we choose softmax cross entropy
loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=y_target))

# Create a prediction function
prediction = tf.nn.softmax(model_output)
val_prediction = tf.nn.softmax(val_model_output)
# Create accuracy function
def get_accuracy(logits, targets):
    batch_predictions = np.argmax(logits, axis=1)
    num_correct = np.sum(np.equal(batch_predictions, targets))
    return(100. * num_correct/batch_predictions.shape[0])

# Start a graph session
sess = tf.Session()

# Create an optimizer
my_optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)
train_step = my_optimizer.minimize(loss)

# Initialize Variables (Variables需要初始化才能用)
init = tf.global_variables_initializer()
sess.run(init)

"""### 模型訓練開始"""

# Training
train_loss = []
train_acc = []
test_acc = []
for i in range(generation):
    rand_index = np.random.choice(len(trainX), size=batch_size)
    rand_x = trainX[rand_index]
    rand_x = np.expand_dims(rand_x, 3)
    rand_y = trainY[rand_index] # label
    train_dict = {x_input: rand_x, y_target: rand_y}
    
    sess.run(train_step, feed_dict=train_dict)
    temp_train_loss, temp_train_preds = sess.run([loss, prediction], feed_dict=train_dict)
    temp_train_acc = get_accuracy(temp_train_preds, rand_y)
    # 每 eval_every 步 即利用trainVal的資料來看一下模型準確率
    if (i+1) % eval_every == 0:
        eval_index = np.random.choice(len(trainValX), size=evaluation_size)
        eval_x = trainValX[eval_index]
        eval_x = np.expand_dims(eval_x, 3)
        eval_y = trainValY[eval_index]
        test_dict = {eval_input: eval_x, eval_target: eval_y}
        test_preds = sess.run(val_prediction, feed_dict=test_dict)
        temp_test_acc = get_accuracy(test_preds, eval_y)
        
        # Record and print results
        train_loss.append(temp_train_loss)
        train_acc.append(temp_train_acc)
        test_acc.append(temp_test_acc)
        acc_and_loss = [(i+1), temp_train_loss, temp_train_acc, temp_test_acc]
        acc_and_loss = [np.round(x,2) for x in acc_and_loss]
        print('Generation # {}. Train Loss: {:.2f}. Train Acc (Val Acc): {:.2f} ({:.2f})'.format(*acc_and_loss))